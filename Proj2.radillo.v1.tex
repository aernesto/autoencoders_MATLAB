\documentclass{article}
%\usepackage[utf8]{inputenc}
\usepackage{amssymb,amsmath, hyperref}
\usepackage{fancyhdr}
\usepackage[title]{appendix}
\pagestyle{fancy}
\fancyhf{}
\rhead{Math - 6373}
\lhead{Project 2}
\chead{Autoencoders}
\rfoot{Page \thepage}
\lfoot{A. Radillo}
\usepackage{graphicx}
\usepackage{xcolor}
\title{Autoencoders training for handwritten digits classification \\ {\large Project 2 - Math-6373 - Prof. Azencott}}
\author{Adrian Radillo - PSID: 1328335}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\makeatletter
\g@addto@macro\@floatboxreset\centering
\makeatother

\usepackage{tabularx,ragged2e,booktabs,caption}
\newcolumntype{C}[1]{>{\Centering}m{#1}}
\renewcommand\tabularxcolumn[1]{C{#1}}


\usepackage{listings}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
%\usepackage{mdframed}
\begin{document}


\lstset{language=Matlab,%
    %basicstyle=\color{red},
    breaklines=true,%
    morekeywords={matlab2tikz},
    keywordstyle=\color{blue},%
    morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
    identifierstyle=\color{black},%
    stringstyle=\color{mylilas},
    commentstyle=\color{mygreen},%
    showstringspaces=false,%without this there will be a symbol in the places where there is a space
    numbers=left,%
    numberstyle={\tiny \color{black}},% size of the numbers
    numbersep=9pt, % this defines how far the numbers are from the text
    emph=[1]{for,end,break},emphstyle=[1]\color{red}, %some words to emphasise
    %emph=[2]{word1,word2}, emphstyle=[2]{style},    
}


\maketitle
\section{Database}
I use the task and data as in project 1. This is a set of
60,000 14x14 grayscale images of handwritten digits for the training set and
10,000 images for the test set, all taken from the MNIST database and pre-processed by myself
in project 1.

\section{Autoencoder architecture}
\begin{itemize}
\item The input and output layers have $14\times14=196$ units. The range of each one of these units  
is $[0,1]$ as the initial pixel intensities ranging from 0 to 255 were rescaled by 1/255.
\item My tentative values for $h$ are $50,100,150$ for the $h<n_1$ case, and $h=250,300,350$ for the $h>n_1$ case.
\end{itemize}

\section{First experiment}
\begin{itemize}
\item My ST is MATLAB, and more specifically, the Neural Network Toolbox from the R2015b version. I created custom neural networks for the three small autoencoders with the function create\_NN() presented in appendix~\ref{app:create}.
\item In figure~\ref{fig:trainFcn} are short and non-exhaustive descriptions of the options available in this toolbox.
\begin{description}
\item[learning] A neural network object has a property called trainFcn. This object property can be set to many 
different values, which correspond to a variety of learning algorithms. The backpropagation gradient descent
algorithm corresponds to the name `traingd'. However, in order to update the weights after the presentation of a whole batch I believe that the name `trainb' is more appropriate. But I really struggled to find clear documentation on this
point. Below is a list of all the other options offered by the ST:
\begin{figure}
\centering
\fbox{\includegraphics[width=\textwidth]{trainFcn.png}}
\caption{Existing built-in training functions in my ST. Taken from~\cite{Demuth2006}}\label{fig:trainFcn}
\end{figure}
\item[initialization] I used the `rands' initFcn property value for both the weights and the biases. This samples 
independent values from the uniform distribution over the interval $[-1,1]$ for each weight and bias.
\begin{figure}
\centering
\fbox{\includegraphics[width=\textwidth]{initFcn.png}}
\caption{Existing built-in initializing functions in my ST. Taken from~\cite{Demuth2006}. This picture only concerns the inputWeights property but `rands' exists as well for the layerWeights and biases properties.}
\end{figure}
\item[batch learning] The ideal batch learning option that would have suited my needs in the MATLAB Neural Network Toolbox is the `trainingOptions' function called for a convolutional neural network object with the stochastic gradient descent with momentum (`sgdm') solver in the R2016b release of MATLAB. However, my version of MATLAB didn't have this option,
so I set out to produce my training batches myself (as in project 1).

I produced 3,000 batches containing 500 cases each, taken from the 60,000 cases in the training set. Consecutive
batches were constructed with an overlap of 100 cases, and the whole training set was used 20 times in order to 
produce the batches. Each sweep through the training set was preceded by a shuffling of its elements order.
I refer you to the make\_batches.m function in the appendix of my project 1 to see the source code of this function. 
In appendix~\ref{app:train_diabolo} I show how the train() function from my ST was called sequentially for each batch
in order to train my autoencoder.
\item[step size] The step size is controlled by the `lr' property of trainFcn property. The ST offered a learning algorithm
based on gradient descent with adapting learning rate (`traingda'), but I was not sure that this corresponded to 
the equations seen in class. The `traingd' learning function, that I used, uses a fixed learning rate.
Since I called the train() function sequentially on each batch, I only set the number of Epochs in the training properties to 1, for each iteration. Then, in between iterations, I wasn't sure whether I should change the learning rate manually, or
if the training algorithm would do it for me. This is why there are a lot of commented lines in appendix~\ref{app:train_diabolo}.
\end{description}
\end{itemize}
\section{Training `small' autoencoders}
\subsection{Implementation of the standard retro-propagation rule}
Below is a list of mathematical notation that I used to derive the retro-propagation rule by hand.
The final result is expressed in equations~\eqref{one} and~\eqref{two}.
\begin{itemize}
\item Matrix of weights between input and hidden layer:
\[
W=\left(w_{ij}^{(1)}\right)_{ij}, \quad 1\leq i\leq n_2; \quad1\leq j \leq n_1+1,
\]
where $w_{in_1+1}^{(1)}$ is always the threshold of unit $i$.
\item Matrix of weights between hidden and output layer:
\[
W=\left(w_{ij}^{(2)}\right)_{ij}, \quad 1\leq i\leq n_3; \quad1\leq j \leq n_2+1,
\]
where $w_{in_2+1}^{(1)}$ is always the threshold of unit $i$.
\item The total number of cases in the training set or in the batch is denoted by $M$.
\item The state of input unit $i$ under presentation of case $m$ is denoted $x_i$. 
\item The state of hidden unit $j$ under presentation of case $m$ is denoted $h_j(W,m)$.
\item The state of unit $k$ on the output layer under presentation of case $m$ is denoted $o_k(W,m)$.
Hence, the output units states are: $o_1(W,m),\ldots,o_{n_3}(W,m)$.
\item Linear output of hidden unit $i$:
\[
A_i^{(1)}(W,m)=\sum_{j=1}^{n_1+1}x_jw_{ij}^{(1)}
\]
\item Logistic function is denoted $\sigma$. We have:
\[
\sigma(v)=\frac{1}{1+e^{-v}}\qquad \sigma'(v)=\frac{e^{-v}}{\lp1+e^{-v}\rp^2}
\]
\item Output of unit $j$ in hidden layer, under presentation of case $m$:
\[
h_j(W,m)=\sigma\lp A_j^{(1)}(W,m) \rp
\]
\item Linear output of the output layer unit $i$:
\[
A_i^{(2)}(W,m)=\sum_{j=1}^{n_2+1}h_j(W,m)w_{ij}^{(2)}
\]
\item Output of unit $i$ in output layer, under presentation of case $m$:
\[
o_i(W,m)=\sigma\lp A_i^{(2)}(W,m) \rp
\]
\item Denote by $\delta_{x,y}$ the Kronecker delta which is 1 only when $x=y$ and zero otherwise.
\item Also, define $\iota$ to be the function that maps each case $m$ to the output unit index $\iota(m)$
which codes for the correct label of this case. Hence, if case 36 represents a 4, then unit $o_5$ will code
for it in $\text{OUT}_{36}$ and therefore $\iota(36)=5$.
\item We denote the MSE function by the letter $f$:
\begin{align}
f(W)&=\frac{1}{M}\sum_{m=1}^{M}\left |\left| \widehat{\text{OUT}}_m-\text{OUT}_m \right |\right |_2^2\\
	&=\frac{1}{M}\sum_{m=1}^M \sum_{k=1}^{n_3}\lp o_{k}(W,m)-\delta_{k,\iota(m)}\rp^2
\end{align}
\item The update rule for the weights is:
\[
\Delta W_n=-\text{grad} \lp f(W)\rp\cdot \frac{\gamma}{\epsilon_n}
\]
\end{itemize}
The formula for the partial derivative of the MSE with respect to a weight from the H-OUT layer, $w^{(2)}_{ij}$, is:
\begin{equation}
\frac{\partial f}{\partial w^{(2)}_{ij}}(W)=\frac{2}{M}\sum_{m=1}^M\sigma'\left(A_i^{(2)}(W,m)\right)h_j(W,m)\left[o_i(W,m)-\delta_{\iota(m),i}\right] \label{one}
\end{equation}
And when it is with respect to a weight from the IN-to-H layer, $w^{(1)}_{ij}$, we get:
\begin{equation}
\frac{\partial f}{\partial w^{(1)}_{ij}}(W)=\frac{2}{M}
\sum_{m=1}^M\left[\sigma'\left(A_i^{(1)}(W,m)\right)x_j
\sum_{k=1}^{n_3}\sigma'\left(A_k^{(2)}(W,m)\right) w^{(2)}_{ki}\left[o_k(W,m)-\delta_{\iota(m),k}\right] \right]\label{two}
\end{equation}

\section{Second experiment: `large' autoencoders with sparsity}
I trained the `large autoencoders' with the code presented in appendix~\ref{app:sparse}
\subsection{Results}
The following table was generated with the script presented in appendix~\ref{app:msemat} after training the 9 autoencoders.\vspace{0.8cm}\\
{\centering
\begin{tabular}{l | c  cccccccc}
MSE& $h=50$&$h=100$&$h=150$&
$h=250$&$h=300$&$h=350$&
$h=250$&$h=300$&$h=350$\\ 
table& &&&
$ \rho=0.05$&$\rho=0.05$&$ \rho=0.05$&
$ \rho=0.15$&$\rho=0.15$&$\rho=0.15$\\ 
\hline
training & 0.3485  &  0.3313  &  0.3385  &  0.0742  &  0.0744  &  0.0731  &  0.0720  &  0.0680  &  0.0647\\
test &  0.3495   & 0.3314  &  0.3367   & 0.0735 &   0.0737  &  0.0725   & 0.0714 &   0.0674  &  0.0638
\end{tabular}}
\section{Detailed analysis of hidden layer structure and efficiency}
I did not have time to do this part of the homework.
\bibliographystyle{plain}
\bibliography{auto}
\begin{appendices}
\section{MATLAB code}
\subsection{Custom network with NN Toolbox in MATLAB}
\label{app:create}
\lstinputlisting{create_NN.m}
\subsection{Training the `diabolo'-autoencoders}
\label{app:train_diabolo}
\lstinputlisting{train_autoencoder.m}
\subsection{Training autoencoders with sparsity constraint}
\label{app:sparse}
\lstinputlisting{train_sparse.m}
\begin{center}
\line(1,0){250}
\end{center}
\lstinputlisting{train_sparse_UH.m}
\subsection{Generating matrix of MSE on training and test sets for all autoencoders}
\label{app:msemat}
\lstinputlisting{mse_perf_small.m}
\end{appendices}
\end{document}
